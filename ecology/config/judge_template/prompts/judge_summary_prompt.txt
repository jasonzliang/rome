Attached are results from multiple judges (judge_*.txt) where each judge scored the creativity of answers from different agents (answer_*.txt) on various queries (using 3 metrics: new, surprising, useful). Create an easy to understand yet detailed and informative summary of the scores from each agent:

1. Generate a CSV showing all the scoring results (use example below as an template, but do NOT copy the scores from example):

Query_Category,Agent,Judge,New,Useful,Surprising,Total
constrained_creativity,Cow,Gemini,5,5,5,15
counterfactual_reasoning,Cat,Claude,1,2,3,6
crossdomain_synthesis,Owl,Claude,9,9,8,26
meta_creativity,Dog,Claude,8,8,7,23
openended_creativity,Lion,GPT,8,8,7,23

2. Using easy to read and well formatted tables:
  - Show all of the raw metrics (in a compact table) for all agents on each query
  - Give average score for each of the query answers from each agent.
  - Give average score for each metric (new, surprising, useful) for each agent.
  - Give average score for each agent from each judge.
  - Give bias of each judge toward agent answers generated with same LLM as the judge.
  - For each table, also give overall average for each agent.

3. Finally, analyze what is unique about Caesar agent's answers when compared to the rest. By carefully checking and comparing against other agents, explain how Caesar agent's answer:
  - Structurally different than other answers
  - More creative than other answers
  - Resolves the query in a unique way
  - Any other useful insights

4. IMPORANT: Make sure to double check that your score calculations and analysis are correct. Make sure all calculations are backed up by the raw data and that there are no missing or skipped scores.
