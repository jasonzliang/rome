Attached are results from multiple judges (judge_*.txt) where each judge evaluated the creativity of answers from different agents (answer_*.txt) on various queries (using 3 metrics: new, surprising, useful). Create an easy to understand yet detailed and informative summary of the scores from each agent:

1. Using easy to read and well formatted tables:
  - Show all of the raw metrics (in a compact table) for all agents on each query
  - Give average score for each of the query answers from each agent.
  - Give average score for each metric (new, surprising, useful) for each agent.
  - Give average score for each agent from each judge.
  - Give bias of each judge toward agent answers generated with same LLM as the judge.
  - For each table, also give overall average for each agent.

2. Finally, analyze what is unique about Caesar agent's answers when compared to the rest. By carefully checking and comparing against other agents, explain how Caesar agent's answers are structurally different. Give as many useful insights as you can.
3. IMPORANT: Make sure to double check that your score calculations and analysis are correct. Make sure all calculations are backed up by the raw data and that there are no missing or skipped scores.